{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is downloadable [here](https://engineering.purdue.edu/LASCI/research-data/outages/outagerisks).\n",
    "\n",
    "A data dictionary is available at this [article](https://www.sciencedirect.com/science/article/pii/S2352340918307182) under *Table 1. Variable descriptions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "### Introduction\n",
    "- We are given outages dataset with region, timing, causality... etc.\n",
    "\n",
    "### Cleaning and EDA\n",
    "Cleaning: \n",
    "- First, we removed the row 'units' and put it into a dictionary with keys being the column title and the value as the corresponding unit. We removed the the units row completely from the dataframe. Then, we removed the rows and columns with completely empty values then set the column name to the appropriate title. There were two main columns (CUSTOMERS.AFFECTED, OUTAGE.DURATION) we were considering when considering EDA and hypothesis test, so we imputed the null values assuming NERC.REGION is MAR for both columns (because regional facilities are most likely a factor on how many people in that region is affected and how long the outage is solved.) with the grouped mean. Later, we combined date and time columns into one and dropped the excess. \n",
    "\n",
    "EDA\n",
    "- While we were exploring the columns of the data we found a really interesting statistic that one category was dominating one column more than the other categories within that column and then we decided to look more into what were the differences between this category than the others on what kind of outages were being caused. However, we changed the topics a bit to include more of the data so we compared the causes of outages with the type of outages they were causing, the regular outages and our criteria of more severe outages.\n",
    "\n",
    "### Assessment of Missingness\n",
    "- First, we determine the NMAR of the columns we chose. To see the missingness of columns, we took two categorical columns with non trivial missing values and utilized TVDS permutation test to determine the conditonality. Took column OUTAGE.RESTORATION compared to CUSTOMER.GROUP(self created) to see its missingness, we found that it is not conditional. Took column CLIMATE.CATEGORY compared to CAUSE.CATEGORY to see its missingness, we found that it is conditional.\n",
    "\n",
    "### Hypothesis Test\n",
    "\n",
    "- For our null hypothesis, we said that CAUSE.CATEGORY did not have a direct affect on whether or not a more severe outage would occur. While the alternate hypothesis, was that CAUSE.CATEGORY did have a direct affect on whether or not a more severe outage would occur. We tested this by calculating the TVD of the CAUSE.CATEGORY, since it is a categorical variable, with whether the outage is severe or not. In the end, we rejected the null hypothesis the p-value basically stated that it is impossible to get the observed test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outages = pd.read_excel('outage.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In units, I pulled out the unit of each column title, so if you have any question on what the unit of each column is,\n",
    "type in the name of the column into the key of dictionary and the value will tell you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting units for each column\n",
    "units = {k:v for (k,v) in zip(outages.iloc[4].values, outages.iloc[5].values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I dropped the rows and columns with no values and set the column title with appropriate row. In the EDA and our hypothesis test we will have specific concern over the column CUSTOMERS.AFFECTED and OUTAGE.DURATION, so we filled the nan value with relevant numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of the data\n",
    "outages.columns = outages.iloc[4].values # setting column name to the correct row\n",
    "outages_cleaned = outages.drop([0,1,2,3,4,5]) #dropped empty rows that holds no value or data\n",
    "outages_cleaned = outages_cleaned.reset_index(drop = True) #resetting index for clarity\n",
    "\n",
    "#dropping column with number of observations and 'variables that holds no meaning as a column\n",
    "outages_cleaned = outages_cleaned.drop(columns = ['OBS','variables']) \n",
    "\n",
    "# fillna CUSTOMERS.AFFECTED with mean of grouped NER.REGION mean due to MAR between two columns\n",
    "outages_cleaned['CUSTOMERS.AFFECTED'] = outages_cleaned['CUSTOMERS.AFFECTED'].astype(float)\n",
    "region_grouped_cust_mean = outages_cleaned.groupby('NERC.REGION')['CUSTOMERS.AFFECTED'].transform(np.mean)\n",
    "outages_cleaned['CUSTOMERS.AFFECTED'] = outages_cleaned['CUSTOMERS.AFFECTED'].fillna(region_grouped_cust_mean)\n",
    "\n",
    "# fillna OUTAGE.DURATION with mean of grouped NER.REGION mean due to MAR between two columns\n",
    "outages_cleaned['OUTAGE.DURATION'] = outages_cleaned['OUTAGE.DURATION'].fillna(-1)\n",
    "outages_cleaned['OUTAGE.DURATION'] = outages_cleaned['OUTAGE.DURATION'].astype(float)\n",
    "region_grouped_rest_dur_mean = outages_cleaned.groupby('NERC.REGION')['OUTAGE.DURATION'].transform(np.mean)\n",
    "outages_cleaned['OUTAGE.DURATION'] = outages_cleaned['OUTAGE.DURATION'].fillna(region_grouped_rest_dur_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
